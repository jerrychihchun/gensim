{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = ['Forgiveness means letting go of the hope for a better past.',\n",
    "       'Good things come to those who wait; better things come to those who work for it.',\n",
    "       'At the end of the day, it\\'s all about the person you want to own a dog with.',\n",
    "       'If you are irritated by every rub, how will your mirror be polished?']\n",
    "\n",
    "doc2 = ['Modern science explicitly and emphatically rejects teleology.',\n",
    "        'Too bad all the people who know how to run the country are busy driving taxi cabs and cutting hair.',\n",
    "       'Language is a system of conventional signs that can be voluntarily produced at any time.',\n",
    "       'History is nothing whatever but a record of what living persons have done in the past.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[text for text in sentence.split()] for sentence in doc]\n",
    "texts2 = [[text for text in sentence.split()] for sentence in doc2]\n",
    "dictionary = corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.add_documents(texts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "doc = ['Rose is a rose is a rose',\n",
    "       'How much wood would a woodchuck chuck if a woodchuck could chuck wood?',\n",
    "       'She sells seashells by the seashore',\n",
    "       'You know New York, you need New York, you know you need unique New York']\n",
    "tokens = [simple_preprocess(sentence) for sentence in doc]\n",
    "\n",
    "mydict = corpora.Dictionary()\n",
    "mycorpus = [mydict.doc2bow(doc, allow_update = True) for doc in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mycorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = [[(mydict[id], count) for id, count in line] for line in mycorpus] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict.save('mydict.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict = corpora.Dictionary.load('mydict.dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "import numpy as np\n",
    "doc = ['Forgiveness means letting go of the hope for a better past.',\n",
    "       'Good things come to those who wait; better things come to those who work for it.',\n",
    "       'At the end of the day, it\\'s all about the person you want to own a dog with.',\n",
    "       'If you are irritated by every rub, how will your mirror be polished?']\n",
    "\n",
    "doc2 = ['Modern science explicitly and emphatically rejects teleology.',\n",
    "        'Too bad all the people who know how to run the country are busy driving taxi cabs and cutting hair.',\n",
    "       'Language is a system of conventional signs that can be voluntarily produced at any time.',\n",
    "       'History is nothing whatever but a record of what living persons have done in the past.']\n",
    "document = [' '.join(doc), ' '.join(doc2)]\n",
    "mydict = corpora.Dictionary([simple_preprocess(line) for line in document])\n",
    "corpus = [mydict.doc2bow(simple_preprocess(line)) for line in document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in corpus:\n",
    "    print([[mydict[id], freq] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus, smartirs = 'ntc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in tfidf[corpus]:\n",
    "    print([[mydict[id], np.around(freq, decimals = 2)] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import LdaModel, LdaMulticore\n",
    "from gensim.utils import simple_preprocess, lemmatize\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words = stop_words + ['com', 'edu', 'subject', 'lines', 'organization', 'would', 'article', 'could']\n",
    "dataset = api.load(\"text8\")\n",
    "data = [d for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_processed = []\n",
    "\n",
    "for i, doc in enumerate(data[:100]):\n",
    "    doc_out = []\n",
    "    for word in doc:\n",
    "        if word not in stop_words:\n",
    "            lemma = lemmatize(word, allowed_tags = re.compile('(NN|JJ|RB)'))\n",
    "            if lemma:\n",
    "                doc_out += [lemma[0].split(b'/')[0].decode('utf-8')]\n",
    "        else:\n",
    "            continue\n",
    "    data_processed.append(doc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(data_processed)\n",
    "corpus = [dictionary.doc2bow(line) for line in data_processed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Latent Dirichlet Allocation \n",
    "lda_model = LdaMulticore(corpus = corpus, \n",
    "                        id2word=dictionary,\n",
    "                        random_state = 100,\n",
    "                        num_topics = 7,\n",
    "                        passes = 10,\n",
    "                        chunksize = 100,\n",
    "                        batch = False,\n",
    "                        alpha = 'asymmetric',\n",
    "                        decay = 0.5,\n",
    "                        offset = 64,\n",
    "                        eta = None,\n",
    "                        eval_every = 0,\n",
    "                        iterations = 100,\n",
    "                        gamma_threshold = 0.001,\n",
    "                        per_word_topics = True)\n",
    "\n",
    "#lda_model.save('lda_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.print_topics(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in lda_model[corpus[10:13]]:\n",
    "    print(\"Document Topics      : \", c[0])      # [(Topics, Perc Contrib)]\n",
    "    print(\"Word id, Topics      : \", c[1][:3])  # [(Word id, [Topics])]\n",
    "    print(\"Phi Values (word id) : \", c[2][:2])  # [(Word id, [(Topic, Phi Value)])]\n",
    "    print(\"Word, Topics         : \", [(dictionary[word], topic) for word, topic in c[1][:2]])   # [(Word, [Topics])]\n",
    "    print(\"Phi Values (word)    : \", [(dictionary[word], topic) for word, topic in c[2][:2]])  # [(Word, [(Topic, Phi Value)])]\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Latent Semantic Analysis\n",
    "from gensim.models import LsiModel\n",
    "lsi_model = LsiModel(corpus = corpus, id2word = dictionary, num_topics = 7, decay = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lsi_model.print_topics(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_model[corpus[10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "import gensim.downloader as api\n",
    "\n",
    "dataset = api.load('text8')\n",
    "data = [d for d in dataset]\n",
    "\n",
    "data_pt1 = data[:1000]\n",
    "data_pt2 = data[1000:]\n",
    "\n",
    "model_word2vec = Word2Vec(data_pt1, min_count = 0, workers = cpu_count())\n",
    "\n",
    "#model_word2vec.save('model_word2vec')\n",
    "#model_word2vec = Word2Vec.load('model_word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word2vec['science'] #size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word2vec.most_similar('science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word2vec.build_vocab(data_pt2, update = True) #Update an existing Word2Vec model\n",
    "model_word2vec.train(data_pt2, total_examples = model.corpus_count, epochs = model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word2vec['science']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word2vec.most_similar('science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Pre-trained models\n",
    "#fasttext = api.load('fasttext-wiki-news-subwords-300')\n",
    "#word2vec = api.load('word2vec-google-news-300')\n",
    "#glove = api.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fasttext.doesnt_match(['india', 'germany', 'china', 'beer']))\n",
    "print(fasttext.distance('king','queen'))\n",
    "print(fasttext.distances('king', ['queen', 'man', 'woman']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fasttext.cosine_similarities(model['king'],\n",
    "                               vectors_all = (model['queen'], model['man'], model['woman'], \n",
    "                                             model['queen'] + model['man'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fasttext.words_closer_than('king', 'queen')) #to word 1 than to word 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fasttext.most_similar(positive = 'king', negative = None, topn = 5,\n",
    "                                 restrict_vocab = None, indexer = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiplicative combination object\n",
    "print(fasttext.most_similar_cosmul(positive = 'king', negative = None, topn = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "dataset = api.load('text8')\n",
    "data = [d for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_document(list_of_list_of_words):\n",
    "    for i, list_of_words in enumerate(list_of_list_of_words):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i]) \n",
    "        #use yield when we want to iterate over a sequence, but don’t want to store the entire sequence in memory\n",
    "train_data = list(tag_document(data)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size = 50, min_count = 2, epochs = 40)\n",
    "model.build_vocab(train_data)\n",
    "model.train(train_data, total_examples = model.corpus_count, epochs = model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass a sentence as a list of words\n",
    "model.infer_vector('German chancellor to appear in the G20 Summit'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.matutils import softcossim\n",
    "from gensim import corpora\n",
    "\n",
    "sent1 = 'The chancellor of Germany is a chemist.'.split()\n",
    "sent2 = 'Angela Merkel is a chemist.'.split()\n",
    "sent3 = 'She is a chemist.'.split()\n",
    "#similarity_matrix = fasttext.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)\n",
    "documents = [sent1, sent2, sent3]\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "sent1 = dictionary.doc2bow(sent1)\n",
    "sent2 = dictionary.doc2bow(sent2)\n",
    "sent3 = dictionary.doc2bow(sent3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(softcossim(sent1, sent2, similarity_matrix))\n",
    "print(softcossim(sent1, sent3, similarity_matrix))\n",
    "print(softcossim(sent2, sent3, similarity_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import summarize, keywords\n",
    "\n",
    "text = 'German is a West Germanic language that is mainly spoken in Central Europe. It is the most widely spoken and official or co-official language in Germany, Austria, Switzerland, South Tyrol in Italy, the German-speaking Community of Belgium and Liechtenstein. It is one of the three official languages of Luxembourg and a co-official language in the Opole Voivodeship in Poland. The languages that are most similar to German are the other members of the West Germanic language branch, including Afrikaans, Dutch, English, the Frisian languages, Low German/Low Saxon, Luxembourgish, and Yiddish. There are strong similarities in vocabulary with Danish, Norwegian and Swedish, although those belong to the North Germanic group. German is the second most widely spoken Germanic language, after English. One of the major languages of the world, German is a native language to almost 100 million people worldwide and the most widely spoken native language in the European Union. German is the third most commonly spoken foreign language in the EU after English and French, making it the second biggest language in the EU in terms of overall speakers. German is also the second most widely taught foreign language in the EU after English at primary school level (but third after English and French at lower secondary level), the fourth most widely taught non-English language in the US (after Spanish, French and American Sign Language), and the second most commonly used scientific language as well as the third most widely used language on websites after English and Russian. The German-speaking countries are ranked fifth in terms of annual publication of new books, with one tenth of all books (including e-books) in the world being published in the German language. In the United Kingdom, German and French are the most sought-after foreign languages for businesses (with 49% and 50% of businesses identifying these two languages as the most useful, respectively).'\n",
    "\n",
    "print(text + '\\n')\n",
    "print(summarize(text, split = True, word_count = 50, ratio = 0.4)) \n",
    "#split: a list of strings; word_count: maximum; ratio: default at 20%\n",
    "print(keywords(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
